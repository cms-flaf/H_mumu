import os
import pickle as pkl
from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import tomllib
import torch
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm


class Trainer:

    def __init__(
        self,
        training_data,
        validation_data,
        hyperparams,
        batch_size,
        epochs,
        early_stop=False,
        patience=None,
        early_threshold=0,
        device=None,
    ):

        # Training data as a tuple of NumPy arrays (x_data, y_data)
        self.training_data = training_data
        # Convert the data to a Torch DataLoader, for optimal training
        self.train_dataloader = self._make_dataloader(training_data, batch_size, device)
        self.valid_dataloader = self._make_dataloader(
            validation_data, batch_size, device
        )

        self.binary_classification = len(self.training_data[0][1][0]) == 1

        if self.binary_classification:
            self.loss_fn = torch.nn.BCEWithLogitsLoss(reduction="none")
        else:
            #self.loss_fn = torch.nn.NLLLoss(reduction="none")
            self.loss_fn = torch.nn.CrossEntropyLoss(reduction="none")

        self.hyperparams = hyperparams

        self.batch_size = batch_size
        self.epochs = epochs
        self.early_stop = early_stop
        if patience:
            self.patience = patience
        self.threshold = early_threshold

        # Lists to store average epoch losses
        self.training_loss = []
        self.validation_loss = []

    ### Init helpers ###

    def _make_dataloader(self, data, batch_size, device):
        """
        Converts the data (tuple of Numpy arrays) into a DataLoader object
        """
        (x_data, y_data), weights = data
        if device is not None:
            x_data = torch.tensor(x_data, device=device)
            y_data = torch.tensor(y_data, device=device)
            weights = torch.tensor(weights, device=device)
            dataset = TensorDataset(x_data, y_data, weights)
        else:
            dataset = TensorDataset(
                torch.Tensor(x_data), torch.Tensor(y_data), torch.Tensor(weights)
            )
        if batch_size == 0:
            dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)
        else:
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        return dataloader
    
    def _set_optimizer(self, model):
        algo = self.hyperparams['algo']
        hypers = {k : v for k, v in self.hyperparams.items() if k != 'algo'}
        # Case switch
        if algo == 'SGD':
            opt = torch.optim.SGD
        elif algo == 'Adam':
            opt = torch.optim.Adam
        else:
            raise ValueError("Optimizer config should speficy SGD or Adam as algo.")
        # Actually init and set
        self.optimizer = opt(model.parameters(), **hypers)

    ### Utility functions ###

    def plot_losses(self, valid=True, log=False, show=False):
        """
        Creates a scatter plot showing the average epoch loss
        for both training and validation. Saves to file, unless
        show = True
        """
        plt.clf()
        if valid:
            plt.plot(
                self.validation_loss, color="tab:red", marker="o", label="validation"
            )
            filename = "training_loss_plot_with_valid"
        else:
            filename = "training_loss_plot"
        plt.plot(self.training_loss, color="tab:green", marker="o", label="training")
        if self.early_stop:
            i = np.argmin(self.validation_loss)
            plt.scatter(i, self.validation_loss[i], label='min', color='black', marker="D", zorder=6)
        if log:
            plt.yscale("log")
            filename += "_log"
        plt.xlabel("Training epoch")
        plt.ylabel("Avg. loss")
        plt.legend()
        if show:
            plt.show()
        else:
            plt.savefig(filename + ".svg", bbox_inches="tight")


    def write_loss_data(self, output_name="epoch+valid_loss"):
        data = (self.training_loss, self.validation_loss)
        with open(output_name + ".pkl", 'wb') as f:
            pkl.dump(data, f)

    ### Main training functions ###

    def train_single_epoch(self, model):
        """
        The main training loop. Runs a single epoch
        and runs validation
        """
        epoch_loss = 0
        model.train()
        for sample in self.train_dataloader:
            # Every data instance is an input + label pair
            inputs, labels, weights = sample
            # Zero your gradients for every batch!
            self.optimizer.zero_grad()
            # Make predictions for this batch
            outputs = model(inputs)
            # Compute the loss and its gradients
            loss = self.loss_fn(outputs, labels)
            loss = (loss * weights).mean()
            epoch_loss += loss.item()
            loss.backward()
            # Adjust learning weights
            self.optimizer.step()
        # Add epoch training loss data to validator
        avg_loss = epoch_loss / len(self.train_dataloader)
        self.training_loss.append(avg_loss)
        # Run validation
        valid_loss = self.validate(model)
        self.validation_loss.append(valid_loss)
        return model

    def validate(self, model):
        """
        Runs the model on non-training data to evaluate progress
        """
        total_loss = 0
        model.eval()
        with torch.no_grad():
            for sample in self.valid_dataloader:
                x, y, weight = sample
                guess = model(x)
                loss = self.loss_fn(guess, y)
                loss = (weight * loss).mean()
                total_loss += loss.item()
        avg_loss = total_loss / len(self.valid_dataloader)
        return avg_loss

    ### Training loop definitions ###

    def train_fixed(self, model):
        """
        Runs the training loop for a fixed number of epochs
        """
        for i in tqdm(range(self.epochs), unit='epochs'):
            model = self.train_single_epoch(model)
        return model

    def train_early_stop(self, model):
        """
        Runs the training loop until the validation data performs worse
        """
        best_model = model.state_dict()
        bad_trains = 0
        i = 0
        with tqdm(desc="Training in early stop mode", unit="epochs") as pbar:
            while True:
                model = self.train_single_epoch(model)
                if self.validation_loss[-1] <= min(self.validation_loss) - self.threshold:
                    best_model = model.state_dict()
                    bad_trains = 0
                else:
                    bad_trains += 1
                if bad_trains > self.patience:
                    break
                i += 1
                pbar.update(1)
        model.load_state_dict(best_model)
        return model

    ### Main (call this function) ###

    def train(self, model):
        self._set_optimizer(model)
        if self.early_stop and self.patience is not None:
            model = self.train_early_stop(model)
        else:
            model = self.train_fixed(model)
        print("Training finished!")
        return model
